{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸ¤Ÿ Sign Language Translator - VERSIÃ“N COMPLETA\n",
    "\n",
    "## âœ¨ CaracterÃ­sticas Completas\n",
    "- ðŸ“¹ **Webcam en Gradio** (dentro de la interfaz)\n",
    "- ðŸŽ¥ **Captura mÃºltiple** con detecciÃ³n de manos\n",
    "- ðŸŽ“ **Entrenamiento** con Bi-LSTM + Attention\n",
    "- ðŸ”® **TraducciÃ³n** en tiempo real\n",
    "- ðŸ“Š **240 Features** por frame\n",
    "- ðŸ”„ **Data Augmentation**\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ“‹ Flujo Completo\n",
    "\n",
    "### 1. Captura (Tab 1)\n",
    "- Webcam en vivo en Gradio\n",
    "- Click \"Preview\" â†’ Ver mano + keypoints\n",
    "- Click \"Capturar\" â†’ Graba mÃºltiples muestras\n",
    "- Auto-pausa sin manos\n",
    "\n",
    "### 2. Entrenamiento (Tab 2)\n",
    "- Carga datos capturados\n",
    "- Entrena Bi-LSTM + Attention\n",
    "- Data augmentation automÃ¡tico\n",
    "- Guarda mejor modelo\n",
    "\n",
    "### 3. TraducciÃ³n (Tab 3)\n",
    "- Carga modelo entrenado\n",
    "- Webcam en tiempo real\n",
    "- Traduce seÃ±as instantÃ¡neamente\n",
    "- Muestra confianza\n",
    "\n",
    "### 4. Dataset Info (Tab 4)\n",
    "- EstadÃ­sticas de datos\n",
    "- Muestras por seÃ±a\n",
    "- Recomendaciones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# ðŸ”§ CELDA 1: Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"ðŸ¤Ÿ Sign Language Translator - Setup\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# 1. Montar Drive\n",
    "print(\"\\nðŸ“ Paso 1/6: Montando Google Drive...\")\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive', force_remount=True)\n",
    "print(\"âœ… Drive montado\")\n",
    "\n",
    "# 2. Clonar repo\n",
    "print(\"\\nðŸ“¥ Paso 2/6: Clonando repositorio...\")\n",
    "import os\n",
    "os.chdir('/content')\n",
    "\n",
    "GITHUB_REPO = \"https://github.com/Hernan-Mej/sign-language-translator.git\"\n",
    "\n",
    "if os.path.exists('sign-language-translator'):\n",
    "    !cd sign-language-translator && git pull\n",
    "else:\n",
    "    !git clone {GITHUB_REPO}\n",
    "print(\"âœ… Repositorio listo\")\n",
    "\n",
    "# 3. Instalar deps\n",
    "print(\"\\nðŸ“¦ Paso 3/6: Instalando dependencias...\")\n",
    "!pip install -q protobuf==4.25.8\n",
    "!pip install -q -r /content/sign-language-translator/requirements.txt\n",
    "print(\"âœ… Dependencias instaladas\")\n",
    "\n",
    "# 4. Paths\n",
    "print(\"\\nðŸ”§ Paso 4/6: Configurando paths...\")\n",
    "import sys\n",
    "sys.path.insert(0, '/content/sign-language-translator/src')\n",
    "print(\"âœ… Paths configurados\")\n",
    "\n",
    "# 5. Estructura Drive\n",
    "print(\"\\nðŸ“‚ Paso 5/6: Creando estructura...\")\n",
    "from pathlib import Path\n",
    "\n",
    "drive_root = Path('/content/drive/MyDrive')\n",
    "project_dir = drive_root / 'SignLanguageTranslator'\n",
    "\n",
    "for directory in [\n",
    "    project_dir / 'data' / 'raw',\n",
    "    project_dir / 'data' / 'processed',\n",
    "    project_dir / 'models',\n",
    "    project_dir / 'logs',\n",
    "]:\n",
    "    directory.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"âœ… Estructura creada\")\n",
    "\n",
    "# 6. Verificar imports\n",
    "print(\"\\nðŸ§ª Paso 6/6: Verificando mÃ³dulos...\")\n",
    "try:\n",
    "    from config import MODEL_CONFIG, DATA_DIR, MODELS_DIR\n",
    "    from key_points_extractor import KeyPointsExtractor\n",
    "    from data_augmentation import SignLanguageAugmenter\n",
    "    print(\"âœ… MÃ³dulos bÃ¡sicos OK\")\n",
    "    \n",
    "    # Intentar importar training\n",
    "    try:\n",
    "        from advanced_lstm_model import create_model\n",
    "        from training import Trainer\n",
    "        print(\"âœ… MÃ³dulos de entrenamiento OK\")\n",
    "    except ImportError:\n",
    "        print(\"âš ï¸  MÃ³dulos de entrenamiento no encontrados\")\n",
    "        print(\"   Para entrenar, agrega advanced_lstm_model.py y training.py\")\n",
    "except ImportError as e:\n",
    "    print(f\"âŒ Error: {e}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"âœ… SETUP COMPLETADO\")\n",
    "print(\"=\"*70)\n",
    "print(\"\\nðŸ“Œ Ejecuta la celda de UI (abajo)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# ðŸŽ¨ CELDA 2: Interfaz Completa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# INTERFAZ CORREGIDA - PROCESAMIENTO EN TIEMPO REAL UNIFICADO\n",
    "# ============================================================================\n",
    "# Reemplaza la celda de UI completa con este cÃ³digo\n",
    "\n",
    "import gradio as gr\n",
    "import cv2\n",
    "import numpy as np\n",
    "import json\n",
    "import time\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import tensorflow as tf\n",
    "\n",
    "from config import DATA_DIR, MODELS_DIR, MODEL_CONFIG\n",
    "from key_points_extractor import KeyPointsExtractor\n",
    "from data_augmentation import SignLanguageAugmenter\n",
    "\n",
    "# Intentar importar training\n",
    "try:\n",
    "    from advanced_lstm_model import create_model\n",
    "    from training import Trainer\n",
    "    TRAINING_AVAILABLE = True\n",
    "except ImportError:\n",
    "    TRAINING_AVAILABLE = False\n",
    "\n",
    "# ============================================================================\n",
    "# ESTADO GLOBAL\n",
    "# ============================================================================\n",
    "\n",
    "class AppState:\n",
    "    def __init__(self):\n",
    "        self.extractor = KeyPointsExtractor()\n",
    "        self.augmenter = SignLanguageAugmenter()\n",
    "        self.current_model = None\n",
    "        self.sign_map = {}\n",
    "        \n",
    "        # Estado de captura\n",
    "        self.capturing = False\n",
    "        self.current_sign = \"\"\n",
    "        self.total_samples = 0\n",
    "        self.samples_captured = 0\n",
    "        self.frames_per_sample = 30\n",
    "        self.current_sequence = []\n",
    "        \n",
    "        # Buffer para traducciÃ³n\n",
    "        self.translation_buffer = []\n",
    "\n",
    "state = AppState()\n",
    "\n",
    "# ============================================================================\n",
    "# PROCESAMIENTO UNIFICADO\n",
    "# ============================================================================\n",
    "\n",
    "def process_frame_unified(frame, mode=\"preview\"):\n",
    "    \"\"\"\n",
    "    Procesa frame y retorna imagen con keypoints + info.\n",
    "    \n",
    "    Args:\n",
    "        frame: Frame de webcam (RGB)\n",
    "        mode: \"preview\", \"capture\", \"translate\"\n",
    "    \n",
    "    Returns:\n",
    "        processed_frame: Imagen con keypoints dibujados\n",
    "        info_text: Texto informativo\n",
    "    \"\"\"\n",
    "    if frame is None:\n",
    "        return None, \"âš ï¸ No hay frame\"\n",
    "    \n",
    "    # Convertir a BGR para OpenCV\n",
    "    frame_bgr = cv2.cvtColor(frame, cv2.COLOR_RGB2BGR)\n",
    "    \n",
    "    # Extraer keypoints (esto ya dibuja los keypoints en el frame)\n",
    "    processed_frame, features = state.extractor.extract_keypoints(frame_bgr)\n",
    "    \n",
    "    # Detectar mano\n",
    "    hand_detected = not np.all(features == 0)\n",
    "    non_zero = np.count_nonzero(features)\n",
    "    \n",
    "    # Convertir de vuelta a RGB\n",
    "    processed_frame = cv2.cvtColor(processed_frame, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    # Generar info segÃºn modo\n",
    "    if mode == \"preview\":\n",
    "        status = \"âœ… Mano detectada\" if hand_detected else \"âš ï¸ No se detecta mano\"\n",
    "        info = f\"{status}\\\\nðŸ“Š Features: {non_zero}/240\"\n",
    "        \n",
    "    elif mode == \"capture\":\n",
    "        if not state.capturing:\n",
    "            info = \"â¹ï¸ No hay captura activa\\\\n\\\\nClick 'Iniciar Captura' para empezar\"\n",
    "        else:\n",
    "            info = f\"ðŸ“¹ Capturando: {state.current_sign}\\\\n\"\n",
    "            info += f\"ðŸ“Š Muestra: {state.samples_captured + 1}/{state.total_samples}\\\\n\"\n",
    "            info += f\"ðŸ“ Frames: {len(state.current_sequence)}/{state.frames_per_sample}\\\\n\\\\n\"\n",
    "            \n",
    "            if hand_detected:\n",
    "                # Agregar frame a secuencia\n",
    "                state.current_sequence.append(features)\n",
    "                info += \"âœ… Grabando...\\\\n\"\n",
    "                \n",
    "                # Completar muestra\n",
    "                if len(state.current_sequence) >= state.frames_per_sample:\n",
    "                    sign_dir = DATA_DIR / \"raw\" / state.current_sign\n",
    "                    sign_dir.mkdir(parents=True, exist_ok=True)\n",
    "                    \n",
    "                    existing = len(list(sign_dir.glob(\"*.npy\")))\n",
    "                    sequence_array = np.array(state.current_sequence[:state.frames_per_sample])\n",
    "                    \n",
    "                    sample_file = sign_dir / f\"sample_{existing:03d}.npy\"\n",
    "                    np.save(sample_file, sequence_array)\n",
    "                    \n",
    "                    state.samples_captured += 1\n",
    "                    state.current_sequence = []\n",
    "                    \n",
    "                    info += f\"âœ… Muestra {state.samples_captured} guardada!\\\\n\"\n",
    "                    \n",
    "                    if state.samples_captured >= state.total_samples:\n",
    "                        state.capturing = False\n",
    "                        info += f\"\\\\nðŸŽ‰ COMPLETADO!\\\\n\"\n",
    "                        info += f\"ðŸ“¦ {state.samples_captured} muestras en {sign_dir}\\\\n\"\n",
    "                    else:\n",
    "                        info += f\"\\\\nâ¸ï¸ Pausa 1 seg - Cambia Ã¡ngulo\\\\n\"\n",
    "                        time.sleep(1)\n",
    "            else:\n",
    "                info += \"â¸ï¸ Esperando mano...\\\\n\"\n",
    "    \n",
    "    elif mode == \"translate\":\n",
    "        if state.current_model is None:\n",
    "            info = \"âš ï¸ Carga un modelo primero\"\n",
    "        elif not hand_detected:\n",
    "            info = \"â¸ï¸ No se detecta mano\"\n",
    "            state.translation_buffer = []\n",
    "        else:\n",
    "            # Acumular frames\n",
    "            state.translation_buffer.append(features)\n",
    "            \n",
    "            if len(state.translation_buffer) > 30:\n",
    "                state.translation_buffer.pop(0)\n",
    "            \n",
    "            if len(state.translation_buffer) < 30:\n",
    "                info = f\"ðŸ“Š Acumulando: {len(state.translation_buffer)}/30\"\n",
    "            else:\n",
    "                # Predecir\n",
    "                sequence = np.array([state.translation_buffer])\n",
    "                prediction = state.current_model.predict(sequence, verbose=0)\n",
    "                predicted_class = np.argmax(prediction[0])\n",
    "                confidence = prediction[0][predicted_class]\n",
    "                \n",
    "                sign_name = state.sign_map.get(predicted_class, f\"Clase {predicted_class}\")\n",
    "                \n",
    "                info = f\"ðŸ”® SeÃ±a: {sign_name.upper()}\\\\n\"\n",
    "                info += f\"ðŸ“Š Confianza: {confidence:.1%}\\\\n\\\\n\"\n",
    "                \n",
    "                # Top 3\n",
    "                top3_idx = np.argsort(prediction[0])[-3:][::-1]\n",
    "                info += \"Top 3:\\\\n\"\n",
    "                for i, idx in enumerate(top3_idx, 1):\n",
    "                    name = state.sign_map.get(idx, f\"Clase {idx}\")\n",
    "                    conf = prediction[0][idx]\n",
    "                    info += f\"  {i}. {name}: {conf:.1%}\\\\n\"\n",
    "    \n",
    "    return processed_frame, info\n",
    "\n",
    "# ============================================================================\n",
    "# FUNCIONES DE CONTROL\n",
    "# ============================================================================\n",
    "\n",
    "def start_capture(sign_name, num_samples, frames_per_sample):\n",
    "    \"\"\"Inicia captura mÃºltiple.\"\"\"\n",
    "    if not sign_name or not sign_name.strip():\n",
    "        return \"âŒ Ingresa un nombre para la seÃ±a\", gr.update(interactive=False)\n",
    "    \n",
    "    state.capturing = True\n",
    "    state.current_sign = sign_name.strip().lower().replace(\" \", \"_\")\n",
    "    state.total_samples = num_samples\n",
    "    state.frames_per_sample = frames_per_sample\n",
    "    state.samples_captured = 0\n",
    "    state.current_sequence = []\n",
    "    \n",
    "    return f\"âœ… Captura iniciada: '{state.current_sign}'\\\\nðŸŽ¯ Meta: {num_samples} muestras\\\\n\\\\nâ–¶ï¸ Muestra tu mano...\", gr.update(interactive=True)\n",
    "\n",
    "def stop_capture():\n",
    "    \"\"\"Detiene captura.\"\"\"\n",
    "    was_capturing = state.capturing\n",
    "    state.capturing = False\n",
    "    \n",
    "    if was_capturing:\n",
    "        return f\"â¹ï¸ Captura detenida\\\\nðŸ“Š Capturadas: {state.samples_captured}/{state.total_samples}\"\n",
    "    return \"â¹ï¸ No habÃ­a captura activa\"\n",
    "\n",
    "def load_model_for_translation(model_name):\n",
    "    \"\"\"Carga modelo para traducciÃ³n.\"\"\"\n",
    "    try:\n",
    "        if model_name == \"No hay modelos\":\n",
    "            return \"âŒ No hay modelos entrenados\"\n",
    "        \n",
    "        model_path = MODELS_DIR / model_name\n",
    "        if not model_path.exists():\n",
    "            return f\"âŒ Modelo no encontrado: {model_path}\"\n",
    "        \n",
    "        state.current_model = tf.keras.models.load_model(str(model_path))\n",
    "        \n",
    "        # Cargar sign_map\n",
    "        sign_map_path = DATA_DIR / \"processed\" / \"sign_map.json\"\n",
    "        if sign_map_path.exists():\n",
    "            with open(sign_map_path) as f:\n",
    "                state.sign_map = {int(k): v for k, v in json.load(f).items()}\n",
    "        \n",
    "        state.translation_buffer = []\n",
    "        \n",
    "        return f\"âœ… Modelo cargado: {model_name}\\\\nðŸ“Š SeÃ±as: {len(state.sign_map)}\"\n",
    "    except Exception as e:\n",
    "        return f\"âŒ Error: {str(e)}\"\n",
    "\n",
    "def train_model(epochs, batch_size, use_augmentation, progress=gr.Progress()):\n",
    "    \"\"\"Entrena el modelo.\"\"\"\n",
    "    if not TRAINING_AVAILABLE:\n",
    "        return \"âŒ MÃ³dulos de entrenamiento no disponibles\\\\n\\\\nAgrega a tu repo:\\\\n- advanced_lstm_model.py\\\\n- training.py\\\\n\\\\nCÃ³digo en: ALL_FILES_COMPLETE_CODE.md\"\n",
    "    \n",
    "    try:\n",
    "        progress(0, desc=\"Inicializando...\")\n",
    "        \n",
    "        trainer = Trainer(\n",
    "            data_dir=DATA_DIR,\n",
    "            models_dir=MODELS_DIR,\n",
    "            use_augmentation=use_augmentation\n",
    "        )\n",
    "        \n",
    "        progress(0.1, desc=\"Cargando datos...\")\n",
    "        X, y, sign_map = trainer.load_and_prepare_data()\n",
    "        \n",
    "        if len(X) == 0:\n",
    "            return \"âŒ No hay datos. Captura seÃ±as primero (Tab 1).\"\n",
    "        \n",
    "        state.sign_map = sign_map\n",
    "        \n",
    "        progress(0.2, desc=\"Normalizando...\")\n",
    "        X = trainer.normalize_sequence_lengths(X, target_length=30)\n",
    "        \n",
    "        progress(0.3, desc=\"Dividiendo...\")\n",
    "        X_train, X_val, X_test, y_train, y_val, y_test = trainer.split_data(X, y)\n",
    "        \n",
    "        if use_augmentation:\n",
    "            progress(0.4, desc=\"Augmentation...\")\n",
    "            X_train, y_train = trainer.augment_data(X_train, y_train)\n",
    "        \n",
    "        progress(0.5, desc=\"Entrenando...\")\n",
    "        model, history = trainer.train_model(\n",
    "            X_train, y_train,\n",
    "            X_val, y_val,\n",
    "            num_classes=len(sign_map),\n",
    "            epochs=epochs,\n",
    "            batch_size=batch_size,\n",
    "            model_name=\"sign_language_model\"\n",
    "        )\n",
    "        \n",
    "        state.current_model = model\n",
    "        \n",
    "        progress(0.9, desc=\"Evaluando...\")\n",
    "        results = trainer.evaluate_model(model, X_test, y_test, sign_map)\n",
    "        \n",
    "        progress(1.0, desc=\"Completado\")\n",
    "        \n",
    "        output = f\"âœ… ENTRENAMIENTO COMPLETADO\\\\n\\\\n\"\n",
    "        output += f\"ðŸ“Š Dataset:\\\\n\"\n",
    "        output += f\"   SeÃ±as: {len(sign_map)}\\\\n\"\n",
    "        output += f\"   Train: {len(X_train)}\\\\n\"\n",
    "        output += f\"   Val: {len(X_val)}\\\\n\"\n",
    "        output += f\"   Test: {len(X_test)}\\\\n\\\\n\"\n",
    "        output += f\"ðŸ“ˆ Resultados:\\\\n\"\n",
    "        output += f\"   Accuracy: {results['metrics']['accuracy']:.2%}\\\\n\"\n",
    "        output += f\"   Top-3: {results['metrics'].get('top_3_accuracy', 0):.2%}\\\\n\\\\n\"\n",
    "        output += f\"ðŸ’¾ Modelo: sign_language_model_best.h5\\\\n\"\n",
    "        \n",
    "        return output\n",
    "        \n",
    "    except Exception as e:\n",
    "        return f\"âŒ Error: {str(e)}\"\n",
    "\n",
    "def show_dataset_info():\n",
    "    \"\"\"Muestra info del dataset.\"\"\"\n",
    "    raw_dir = DATA_DIR / \"raw\"\n",
    "    if not raw_dir.exists():\n",
    "        return \"ðŸ“­ No hay datos capturados\"\n",
    "    \n",
    "    sign_dirs = [d for d in raw_dir.iterdir() if d.is_dir()]\n",
    "    \n",
    "    if not sign_dirs:\n",
    "        return \"ðŸ“­ No hay seÃ±as capturadas\"\n",
    "    \n",
    "    output = f\"ðŸ“Š DATASET DE SEÃ‘AS\\\\n\" + \"=\"*50 + \"\\\\n\\\\n\"\n",
    "    output += f\"ðŸŽ¯ Total de seÃ±as: {len(sign_dirs)}\\\\n\\\\n\"\n",
    "    \n",
    "    total_samples = 0\n",
    "    \n",
    "    for sign_dir in sorted(sign_dirs):\n",
    "        samples = list(sign_dir.glob(\"*.npy\"))\n",
    "        num_samples = len(samples)\n",
    "        total_samples += num_samples\n",
    "        \n",
    "        output += f\"ðŸ“ {sign_dir.name}: {num_samples} muestras\"\n",
    "        \n",
    "        if num_samples < 10:\n",
    "            output += \" âš ï¸\\\\n\"\n",
    "        elif num_samples >= 15:\n",
    "            output += \" âœ…\\\\n\"\n",
    "        else:\n",
    "            output += \" âœ“\\\\n\"\n",
    "    \n",
    "    output += \"\\\\n\" + \"=\"*50 + \"\\\\n\"\n",
    "    output += f\"ðŸ“ˆ Total: {total_samples} muestras\\\\n\"\n",
    "    output += f\"ðŸ“Š Promedio: {total_samples / len(sign_dirs):.1f}/seÃ±a\\\\n\"\n",
    "    \n",
    "    if len(sign_dirs) < 5:\n",
    "        output += \"\\\\nðŸ’¡ Captura al menos 5 seÃ±as diferentes\\\\n\"\n",
    "    if total_samples < 75:\n",
    "        output += \"ðŸ’¡ Necesitas ~75 muestras (15Ã—5)\\\\n\"\n",
    "    \n",
    "    return output\n",
    "\n",
    "def get_model_list():\n",
    "    \"\"\"Lista modelos disponibles.\"\"\"\n",
    "    models = [m.name for m in MODELS_DIR.glob(\"*.h5\")]\n",
    "    return models if models else [\"No hay modelos\"]\n",
    "\n",
    "# ============================================================================\n",
    "# INTERFAZ\n",
    "# ============================================================================\n",
    "\n",
    "with gr.Blocks(theme=gr.themes.Soft(), title=\"Sign Language Translator\") as demo:\n",
    "    \n",
    "    gr.Markdown(\"\"\"\n",
    "    # ðŸ¤Ÿ Sign Language Translator\n",
    "    ### âœ¨ Procesamiento en Tiempo Real Unificado\n",
    "    \n",
    "    **Webcam + Keypoints sincronizados â€¢ Captura â€¢ Entrenamiento â€¢ TraducciÃ³n**\n",
    "    \"\"\")\n",
    "    \n",
    "    with gr.Tabs():\n",
    "        \n",
    "        # ====================================================================\n",
    "        # TAB 1: CAPTURA\n",
    "        # ====================================================================\n",
    "        \n",
    "        with gr.Tab(\"ðŸ“¹ Captura\"):\n",
    "            gr.Markdown(\"\"\"\n",
    "            ### Captura con procesamiento en tiempo real\n",
    "            \n",
    "            **La webcam y los keypoints estÃ¡n sincronizados en una sola vista**\n",
    "            \"\"\")\n",
    "            \n",
    "            with gr.Row():\n",
    "                # Webcam con procesamiento en tiempo real\n",
    "                webcam_capture = gr.Image(\n",
    "                    sources=[\"webcam\"],\n",
    "                    streaming=True,\n",
    "                    label=\"Webcam + Keypoints (Tiempo Real)\",\n",
    "                    type=\"numpy\"\n",
    "                )\n",
    "                \n",
    "                # Info en tiempo real\n",
    "                capture_info = gr.Textbox(\n",
    "                    label=\"Estado\",\n",
    "                    lines=12,\n",
    "                    max_lines=12\n",
    "                )\n",
    "            \n",
    "            with gr.Row():\n",
    "                sign_name_input = gr.Textbox(\n",
    "                    label=\"Nombre de la SeÃ±a\",\n",
    "                    placeholder=\"Ej: hola, gracias, adios\"\n",
    "                )\n",
    "                num_samples_input = gr.Slider(1, 30, 15, step=1, label=\"Muestras\")\n",
    "                frames_input = gr.Slider(15, 45, 30, step=5, label=\"Frames/Muestra\")\n",
    "            \n",
    "            with gr.Row():\n",
    "                start_btn = gr.Button(\"â–¶ï¸ Iniciar Captura\", variant=\"primary\")\n",
    "                stop_btn = gr.Button(\"â¹ï¸ Detener\", variant=\"stop\")\n",
    "            \n",
    "            # Stream processing unificado\n",
    "            webcam_capture.stream(\n",
    "                fn=lambda frame: process_frame_unified(frame, mode=\"capture\"),\n",
    "                inputs=webcam_capture,\n",
    "                outputs=[webcam_capture, capture_info],\n",
    "                stream_every=0.1,\n",
    "                time_limit=None\n",
    "            )\n",
    "            \n",
    "            start_btn.click(\n",
    "                fn=start_capture,\n",
    "                inputs=[sign_name_input, num_samples_input, frames_input],\n",
    "                outputs=[capture_info, start_btn]\n",
    "            )\n",
    "            \n",
    "            stop_btn.click(\n",
    "                fn=stop_capture,\n",
    "                outputs=capture_info\n",
    "            )\n",
    "            \n",
    "            gr.Markdown(\"\"\"\n",
    "            **ðŸ’¡ Tips:**\n",
    "            - La vista es Ãºnica: webcam + keypoints en tiempo real\n",
    "            - Auto-pausa cuando no detecta manos\n",
    "            - Captura 15-20 muestras por seÃ±a\n",
    "            - VarÃ­a Ã¡ngulos entre muestras\n",
    "            \"\"\")\n",
    "        \n",
    "        # ====================================================================\n",
    "        # TAB 2: ENTRENAMIENTO\n",
    "        # ====================================================================\n",
    "        \n",
    "        with gr.Tab(\"ðŸŽ“ Entrenamiento\"):\n",
    "            gr.Markdown(\"### Entrenar modelo Bi-LSTM + Attention\")\n",
    "            \n",
    "            with gr.Row():\n",
    "                epochs_input = gr.Slider(10, 200, 100, step=10, label=\"Ã‰pocas\")\n",
    "                batch_size_input = gr.Slider(8, 64, 16, step=8, label=\"Batch Size\")\n",
    "            \n",
    "            augmentation_check = gr.Checkbox(\n",
    "                label=\"Data Augmentation (Recomendado)\",\n",
    "                value=True\n",
    "            )\n",
    "            \n",
    "            train_btn = gr.Button(\"ðŸš€ Entrenar\", variant=\"primary\")\n",
    "            train_output = gr.Textbox(label=\"Resultados\", lines=15)\n",
    "            \n",
    "            train_btn.click(\n",
    "                fn=train_model,\n",
    "                inputs=[epochs_input, batch_size_input, augmentation_check],\n",
    "                outputs=train_output\n",
    "            )\n",
    "            \n",
    "            if not TRAINING_AVAILABLE:\n",
    "                gr.Markdown(\"\"\"\n",
    "                âš ï¸ **MÃ³dulos de entrenamiento no disponibles**\n",
    "                \n",
    "                Agrega a tu repo: `advanced_lstm_model.py` y `training.py`\n",
    "                \n",
    "                CÃ³digo en: `ALL_FILES_COMPLETE_CODE.md`\n",
    "                \"\"\")\n",
    "        \n",
    "        # ====================================================================\n",
    "        # TAB 3: TRADUCCIÃ“N\n",
    "        # ====================================================================\n",
    "        \n",
    "        with gr.Tab(\"ðŸ”® TraducciÃ³n\"):\n",
    "            gr.Markdown(\"\"\"\n",
    "            ### TraducciÃ³n en tiempo real\n",
    "            \n",
    "            **Vista unificada: webcam + keypoints + predicciÃ³n**\n",
    "            \"\"\")\n",
    "            \n",
    "            model_selector = gr.Dropdown(\n",
    "                label=\"Modelo\",\n",
    "                choices=get_model_list(),\n",
    "                interactive=True\n",
    "            )\n",
    "            \n",
    "            load_btn = gr.Button(\"ðŸ“¥ Cargar Modelo\")\n",
    "            load_status = gr.Textbox(label=\"Estado\", lines=2)\n",
    "            \n",
    "            with gr.Row():\n",
    "                # Webcam con procesamiento en tiempo real\n",
    "                webcam_translate = gr.Image(\n",
    "                    sources=[\"webcam\"],\n",
    "                    streaming=True,\n",
    "                    label=\"Webcam + Keypoints + PredicciÃ³n (Tiempo Real)\",\n",
    "                    type=\"numpy\"\n",
    "                )\n",
    "                \n",
    "                # PredicciÃ³n en tiempo real\n",
    "                translate_info = gr.Textbox(\n",
    "                    label=\"PredicciÃ³n\",\n",
    "                    lines=12,\n",
    "                    max_lines=12\n",
    "                )\n",
    "            \n",
    "            load_btn.click(\n",
    "                fn=load_model_for_translation,\n",
    "                inputs=model_selector,\n",
    "                outputs=load_status\n",
    "            )\n",
    "            \n",
    "            # Stream processing unificado\n",
    "            webcam_translate.stream(\n",
    "                fn=lambda frame: process_frame_unified(frame, mode=\"translate\"),\n",
    "                inputs=webcam_translate,\n",
    "                outputs=[webcam_translate, translate_info],\n",
    "                stream_every=0.1,\n",
    "                time_limit=None\n",
    "            )\n",
    "            \n",
    "            demo.load(\n",
    "                fn=lambda: gr.Dropdown(choices=get_model_list()),\n",
    "                outputs=model_selector\n",
    "            )\n",
    "        \n",
    "        # ====================================================================\n",
    "        # TAB 4: DATASET\n",
    "        # ====================================================================\n",
    "        \n",
    "        with gr.Tab(\"ðŸ“Š Dataset\"):\n",
    "            gr.Markdown(\"### InformaciÃ³n del Dataset\")\n",
    "            \n",
    "            refresh_btn = gr.Button(\"ðŸ”„ Actualizar\")\n",
    "            dataset_info = gr.Textbox(label=\"Dataset\", lines=20)\n",
    "            \n",
    "            refresh_btn.click(fn=show_dataset_info, outputs=dataset_info)\n",
    "            demo.load(fn=show_dataset_info, outputs=dataset_info)\n",
    "    \n",
    "    gr.Markdown(\"\"\"\n",
    "    ---\n",
    "    ### ðŸ“š InformaciÃ³n\n",
    "    \n",
    "    **ðŸŽ¥ Mejora clave**: Procesamiento unificado en tiempo real\n",
    "    - Una sola vista: webcam + keypoints sincronizados\n",
    "    - No hay delay entre captura y procesamiento\n",
    "    - Feedback instantÃ¡neo\n",
    "    \n",
    "    **ðŸ’¾ Datos**: `/content/drive/MyDrive/SignLanguageTranslator/`\n",
    "    \"\"\")\n",
    "\n",
    "print(\"\\\\nðŸŽ¨ Iniciando interfaz con procesamiento unificado...\\\\n\")\n",
    "print(\"âœ… Mejoras:\")\n",
    "print(\"   â€¢ Webcam + keypoints en una sola vista\")\n",
    "print(\"   â€¢ Procesamiento en tiempo real sincronizado\")\n",
    "print(\"   â€¢ Sin delay entre captura y visualizaciÃ³n\\\\n\")\n",
    "\n",
    "demo.launch(debug=True, share=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# ðŸ“ Notas\n",
    "\n",
    "## âœ… Mejoras Implementadas\n",
    "\n",
    "### 1. Webcam en Gradio\n",
    "- `gr.Image(sources=[\"webcam\"], streaming=True)`\n",
    "- No necesita JavaScript de Colab\n",
    "- Se ve directamente en la interfaz\n",
    "\n",
    "### 2. Captura Completa\n",
    "- Preview en vivo\n",
    "- Captura mÃºltiple automÃ¡tica\n",
    "- DetecciÃ³n de manos con auto-pausa\n",
    "- Feedback en tiempo real\n",
    "\n",
    "### 3. Entrenamiento Funcional\n",
    "- Carga datos del dataset\n",
    "- Data augmentation\n",
    "- Entrena Bi-LSTM + Attention\n",
    "- Guarda mejor modelo\n",
    "\n",
    "### 4. TraducciÃ³n en Tiempo Real\n",
    "- Carga modelos entrenados\n",
    "- Traduce mientras muestras seÃ±as\n",
    "- Top-3 predicciones\n",
    "- Muestra confianza\n",
    "\n",
    "## ðŸŽ¯ Flujo Completo\n",
    "\n",
    "```\n",
    "1. CAPTURA (Tab 1)\n",
    "   â”œâ”€ Click \"Preview\" â†’ Ver mano + keypoints\n",
    "   â”œâ”€ Ingresar nombre: \"hola\"\n",
    "   â”œâ”€ Muestras: 15\n",
    "   â”œâ”€ Click \"Iniciar Captura\"\n",
    "   â”œâ”€ Mostrar mano â†’ Graba automÃ¡ticamente\n",
    "   â”œâ”€ Sin mano â†’ Auto-pausa\n",
    "   â””â”€ Repetir para 5+ seÃ±as\n",
    "\n",
    "2. ENTRENAMIENTO (Tab 2)\n",
    "   â”œâ”€ Ã‰pocas: 100\n",
    "   â”œâ”€ Batch: 16\n",
    "   â”œâ”€ Augmentation: âœ…\n",
    "   â”œâ”€ Click \"Entrenar\"\n",
    "   â””â”€ Esperar ~10-15 min\n",
    "\n",
    "3. TRADUCCIÃ“N (Tab 3)\n",
    "   â”œâ”€ Seleccionar modelo\n",
    "   â”œâ”€ Click \"Cargar Modelo\"\n",
    "   â”œâ”€ Mostrar seÃ±a a webcam\n",
    "   â””â”€ Ver predicciÃ³n en tiempo real\n",
    "\n",
    "4. DATASET (Tab 4)\n",
    "   â””â”€ Ver estadÃ­sticas y progreso\n",
    "```\n",
    "\n",
    "## âš ï¸ Importante\n",
    "\n",
    "**Para entrenamiento completo necesitas:**\n",
    "- `src/advanced_lstm_model.py`\n",
    "- `src/training.py`\n",
    "\n",
    "**CÃ³digo en**: `ALL_FILES_COMPLETE_CODE.md`\n",
    "\n",
    "**Si no los tienes**, la captura y traducciÃ³n funcionarÃ¡n pero entrenamiento mostrarÃ¡ mensaje de error.\n",
    "\n",
    "---\n",
    "\n",
    "**Â¡Disfruta tu Sign Language Translator completo! ðŸ¤Ÿ**"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
